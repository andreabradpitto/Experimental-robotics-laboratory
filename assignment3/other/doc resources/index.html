<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Experimental robotics laboratory - Assignment 2: Experimental robotics laboratory - Assignment 3</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="dog.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Experimental robotics laboratory - Assignment 2
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Experimental robotics laboratory - Assignment 3 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_other_mainpage"></a> </p>
<h1><a class="anchor" id="autotoc_md1"></a>
Introduction</h1>
<p>This is the repository for the third and last assignment of the Robotics engineering "Experimental robotics laboratory" course, a.y. 2020/2021, held in the University of Genoa. This work is focused on the implemetation of a ROS-based finite state machine (FSM) nested within a more complex architecture. This project has been coded with <a href="https://www.python.org/download/releases/2.7/">Python 2.7</a> and it has been tested on <a href="http://wiki.ros.org/kinetic">ROS Kinetic</a>.</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Setting</h1>
<p>Like for the previous assignment, <a href="https://github.com/andreabradpitto/Experimental-robotics-laboratory/tree/main/assignment2">Assignment 2</a>, the main subject of the project is a robotic dog. This time, it is free to wander inside a house, comprising 6 different rooms: the entrance, the closet, the living room, the kitchen, the bathroom, and the bedroom. Each of these room locations is schematically represented by a colored ball lying on the ground, so that the dog can recognize them. In order to avoid ambiguities, each ball has a different color; following the same order mentioned above, the colors are: blue, red, green, yellow, magenta and black. Event though the house is not procedurally generated (i.e. it is always the same model), every time the simulation is started, the robot does not know where these room are, so it has to learn the house planimetry by exploration. The only thing it knows, indeed, is the association of each colored ball to its respective room, so that when the robotic dog sees e.g. the yellow ball, it knows that that room is the kitchen.<br  />
 <br  />
 Inside the house there is, other than the walls defining each room's border, also a human mannequin model, which represents the dog owner. The robot is in fact listening for human orders, and eager to play with them. The dog, in normal conditions, wanders randomly around the house, looking for undiscovered colored balls; when these are spotted, it reaches them and stores the coordinates of the corresponding room, so that this info can be used later on. When the human asks the robot to play (ideally, by actually telling the robot the word "play"), the dog reaches them, awaits for the human to tell a room name, and then it tries to reach it. Of course, diferent behaviors have been implmented whether the robot already knows the location it has been instructed to reach or not. When the robotic dog reaches the required room, it outputs a vocal acknowledgement, and finally gets back to the owner. When the robot gets tired, it gets back to its charging dock (also referred as "home") and waits for the battery to recharge. This behavior is the real life pet equivalent of sleeping; this parallelism is the one responsible for the state's actual name, as it will be shown later on in this document. The last behavior is the one devoted to the house exploration, for when the robot is looking for a precise still-to-be-discovered ball: this happens we the human wants it to reach a room that is not yet present in its database.<br  />
 <br  />
 The code implemented will show a real-time simulation of the assignment over both <a href="http://gazebosim.org/">Gazebo</a> and <a href="http://wiki.ros.org/rviz">RViz</a>. The human model and the balls will be present only on the former, while the latter is the one that will show the robotic dog's perspective, i.e. all the explored map so far, and the obstacles (the walls or the human model) that it discovered. Gazebo will always show the whole house, so that it is possbile to check if the robot knowledge matches the real planimetry of the environment. In addition, the voice messages shared between the human and the robotic dog will be displayed on terminal.</p>
<h2><a class="anchor" id="autotoc_md3"></a>
The house</h2>
<p>Here is a picture of the house taken from Gazebo:</p>
<div class="image">
<img src="house.png" alt=""/>
</div>
<p>It is comprised inside a 10x10 square, centered a the origin of the three axes. The environment can be thus effectively reduced to a bidimensional scenario, spreading over the xy plane (or, equivalently, on the z = 0 one). The robot initial position, which is also assumed to be its home location, is set in the launch file to be near the human mannequin by default: the latter is in *(-6, 8.5)*, while the former is in *(-5, 8)*. Please notice that in the above picture neither the origin axes nor the floor grid is being shown, in order to only present the house model itself; anyway, Gazebo automatically shows both of them at every new run.<br  />
 In the top left corner of the screenshot, it is possible to see the human mannequin, near another decorative object, both colored to white in order to hinder the robot's room detection feature. The robotic dog is present too, and its model is described in detail in the next subsection.</p>
<h2><a class="anchor" id="autotoc_md4"></a>
The robotic dog</h2>
<p>The robot I implemented is composed by basic geometric shapes, while trying to find a medium between looks and functionality. Indeed, it features two eyes, in order to resemble a real dog, but only one of them holds a sensor, i.e. a camera, which is used to spot the colored balls: the other one has no purpose other than giving some flavour to the model. The other sensor present on the robot is a laser scanner, which is mounted on its main chassis and is used to detect the obstacles in front of it. It is positioned much lower with respect to the camera in order to guarantee the best obstacle detection as possible: the tests I made clearly showed that having the two robot "eyes" as the two sensor spots was much harder to implement successfully.<br  />
 Here is a picture the robotic dog (again, taken from Gazibo):</p>
<div class="image">
<img src="robot.png" alt=""/>
</div>
<p>It is a two-wheeled differential drive robot. Apart from the wheels, all the joints are fixed, so the dog cannot, for intstance, rotate its head to look around. Notice the orange cube on the front, which is the shape representing the above mentioned laser scanner.</p>
<h2><a class="anchor" id="autotoc_md5"></a>
The environments</h2>
<p>Apart from the window showing what the robot sees in real time, this package launches two enviroments: Gazebo, which is used to simulate the physics of the robot, and features nice visuals. The other one is Rviz, which is simply a visualization tool, but that is able to show all the topics involved in the package. One of the most important difference between the two, atleast in this package, is that while in Gazebo one can watch the whole house and the robot with no constraints, in RViz I made so that one only sees what the robot knows and has discovered so far:</p>
<div class="image">
<img src="RViz.png" alt=""/>
</div>
<p>On the left hand side of the image above, there are many the elements useful to track and assess the robot localization, mapping and exploration capabilities. The image in the middle also shows a lot of information:</p><ul>
<li>the black elements are the obstacles detected by the robot</li>
<li>the yellow line is the global navigation plan</li>
<li>the green line is the local navigation plan</li>
<li>the red lines are the current detections of the robot's laser</li>
<li>the blue borders (and the green spheres, not shown here) are elements used by the exploration algorithm (see later on)</li>
<li>the two white squares centered over the robot are the gloabl and local costmaps</li>
</ul>
<h1><a class="anchor" id="autotoc_md6"></a>
Architecture</h1>
<div align="center"> <img src="https://github.com/andreabradpitto/Experimental-robotics-laboratory/blob/main/assignment2/images/architecture%20and%20topics.png" alt="" class="inline"/> </div><p>The architecture is made of five components:</p><ul>
<li><b>Human</b>: it is implemented by <a href="scripts/human.py">human.py</a>, which is used to simulate the dog owner. This component randomly whooses whether the human decides to move the ball along (throw) the playing field, or to hide it</li>
<li><b>Dog FSM</b>: it is implemented by <a href="scripts/dog_fsm.py">dog_fsm.py</a>, which is used to handle the robotic dog's FSM internal architecture</li>
<li><b>Dog control</b>: it is implemented by <a href="scripts/dog_control.py">dog_control.py</a>, which is used to have the robotic dog reaching a random position during the Normal state, or its home during the Sleep state</li>
<li><b>Dog control ball</b>: it is implemented by <a href="scripts/dog_control_ball.py">dog_control_ball.py</a>, which is used to make the dog follow the ball, or to look for it if it has lost track of the green sphere. It also makes the robotic dog turn its head when he perceives that the ball has stopped</li>
<li><b>Go to point ball</b>: it is implemented by <a href="scripts/go_to_point_ball.py">go_to_point_ball.py</a>, and is used to move the ball along the playing field. Goal positions are issued randomly by <a href="scripts/human.py">human.py</a></li>
</ul>
<p>...qua dire gli algoritmi che uso e spiegare explore_lite inglobato... states description</p>
<div align="center"> <img src="https://github.com/andreabradpitto/Experimental-robotics-laboratory/blob/main/assignment2/images/fsm.png" alt="" class="inline"/> </div><p>The dog starts in the <b>Sleep</b> state.<br  />
 Topics involved:</p>
<ul>
<li><code>control_topic</code>: topic used by the FSM to order the <b>Dog_control</b> component to start simulating a movement</li>
<li><code>motion_over_topic</code>: topic whose duty is to inform the FSM when the motion is over or interrupted by the sight of the ball</li>
<li><code>ball_control_topic</code>: topic used by <b>Dog control ball</b> to communicate with <b>Dog FSM</b>: it sends information when the ball is first spotted by the robot, and then when the dog eventually loses track of it</li>
</ul>
<p>The actions used are:</p>
<ul>
<li><code>assignment2.msg/PlanningAction</code>: a simple action whose goals are of type <b>geometry_msgs/PoseStamped</b></li>
</ul>
<p>The message types used are:</p>
<ul>
<li><code>std_msgs.msg/Int64</code>: imported message type consisting in an integer</li>
<li><code>assignment2.msg/Coordinates</code>: message made of two integers <b>x</b> and <b>y</b></li>
</ul>
<p>The latter of the two is of course a custom one, which has been coded for this project and is shipped with this package itself. Standard messages could have been used, but this new type creation had also been created as an extra exercise, in order to get more acquainted with the ROS environment.<br  />
 Finally, here are the parameters (strictly related to the robotic dog) loaded in the ROS parameter server:</p>
<ul>
<li><code>state</code>: parameter specifying robot current state</li>
<li><code>map/x_max</code>: parameter specifying the maximum x-axis value for the map</li>
<li><code>map/y_max</code>: parameter specifying the maximum y-axis value for the map</li>
<li><code>map/x_min</code>: parameter specifying the minimum x-axis value for the map</li>
<li><code>map/y_min</code>: parameter specifying the minimum y-axis value for the map</li>
<li><code>home/x</code>: parameter specifying robot home position (x coordinate)</li>
<li><code>home/y</code>: parameter specifying robot home position (y coordinate)</li>
<li><code>sim_scale</code>: parameter used to scale simulation velocity</li>
<li><code>new_ ball_detected</code>: parameter used to specify whether a new ball has been detected or not</li>
<li><code>unknown_ball</code>: parameter used to identify which ball has to be looked for</li>
<li><code>room_list</code>: list of the available rooms</li>
<li><code>play_task_status</code>: parameter used to specify <b>Play</b> state progress<br  />
 A value of <em>0</em> means that the state is not active or at in initialization phase<br  />
 A <em>1</em> stands for it being in progress<br  />
 A value of <em>2</em> means that it has completed one iteration The last value is also used by the <b>Find</b> state, in case the location of the room asked by the human was not present in the robot's database</li>
<li><code>blue/x</code>: parameter used to specify the x coordinate of the blue ball, once discovered</li>
<li><code>blue/y</code>: parameter used to specify the y coordinate of the blue ball, once discovered</li>
<li><code>red/x</code>: parameter used to specify the x coordinate of the red ball, once discovered</li>
<li><code>red/y</code>: parameter used to specify the y coordinate of the red ball, once discovered</li>
<li><code>green/x</code>: parameter used to specify the x coordinate of the green ball, once discovered</li>
<li><code>green/y</code>: parameter used to specify the y coordinate of the green ball, once discovered</li>
<li><code>yellow/x</code>: parameter used to specify the x coordinate of the yellow ball, once discovered</li>
<li><code>yellow/y</code>: parameter used to specify the y coordinate of the yellow ball, once discovered</li>
<li><code>magenta/x</code>: parameter used to specify the x coordinate of the magenta ball, once discovered</li>
<li><code>magenta/y</code>: parameter used to specify the y coordinate of the magenta ball, once discovered</li>
<li><code>black/x</code>: parameter used to specify the x coordinate of the black ball, once discovered</li>
<li><code>black/y</code>: parameter used to specify the y coordinate of the black ball, once discovered</li>
</ul>
<p>All of these, as already pointed out, can be adjusted before runtime.<br  />
 Finally, here is the list of all the files and folders featured in this package:</p>
<div align="center"> <img src="https://github.com/andreabradpitto/Experimental-robotics-laboratory/blob/main/assignment2/images/tree.png" alt="" class="inline"/> </div><h1><a class="anchor" id="autotoc_md7"></a>
Requirements</h1>
<p>In order to run this piece of software, it is needed to have a Linux distribution that supports (and has them installed):</p><ul>
<li><a href="http://wiki.ros.org/kinetic">ROS Kinetic</a></li>
<li><a href="http://wiki.ros.org/smach">SMACH</a></li>
<li><a href="https://opencv.org/">OpenCV</a></li>
<li>A <a href="https://www.python.org/">Python</a> interpreter</li>
</ul>
<h1><a class="anchor" id="autotoc_md8"></a>
Instructions</h1>
<p>It might be needed to make all the nodes executable, before actually being able to run them. So, after reaching this assignment's folder via a terminal, type:</p>
<div class="fragment"><div class="line">chmod +x scripts/*</div>
<div class="line">chmod +x src/* </div>
</div><!-- fragment --><p>In order to run the code, put the <em>assignment3</em> directory in your workspace, build, then open a terminal and type:</p>
<div class="fragment"><div class="line">roslaunch assignment.launch </div>
</div><!-- fragment --><p>This will start the simulation on both Gazebo and RViz.</p>
<h1><a class="anchor" id="autotoc_md9"></a>
Assumptions, limitations and possible improvements</h1>
<ul>
<li>first of all, due to time constraints, I had no time completely test the code, i.e. I did not leave my program running for hours as suggested (also because my PC would have melted)</li>
<li>during <b>Play</b> the robotic dog ignores (and discards) any new ball discovery: I made this choice in order to grants human orders higher priority with respect to mapping or any other task. That said, I could then have made so that the robot would store its position when a new ball is found during <b>Play</b>, and then get back to it once the state ends</li>
<li>the robot's simulated battery is assumed to drain more rapidly while in <b>Play</b> than when in <b>Normal</b>, single-cycle-wise. This would be actually true in reality most often than not, but of course it would not always be the case. Furthermore, the <b>Find</b> state does not drain battery directly, which is a significant simplification. Anyway, in order to make sure that the robot is always able to get back to its charging station (i.e. home), and also to account for the major battery drain assumption, the robotic dog transitions from <b>Play</b> to <b>Normal</b> after a certain drain threshold is reached, and then always carries out a single <b>Normal</b> state cycle (no matter how far it goes, which is of course another simpification) before going to <b>Sleep</b> to recharge its batteries</li>
<li>the dog is assumed to start in its home position and, more importantly, the position the robot reaches when it gets close to the human (<b>Play</b> state) is again its home, as the mannequin (i.e. the human location) is close the charging station. This is somewhat "hardcoded" in the above mentioned state and, if the mannequin is moved to a location which is far from "home", a couple of lines in the code dog_fsm.py must be changed. In that case, it is thus not sufficient to just adapt the parameters in the sim.launch launch file</li>
<li>if, for some reason, the robot loses track of a ball while it is reaching it, there is no way the robot can recover from there. However, the chances of this happening are very slim. A more realistic issue could be that, again while trying to reach a new ball, the robot could hit a wall, as in that situation it ignores incoming laser data. This happens when a wall corner covers the trajectory from the robot position to the ball, so that the latter is still partially visible from the dog's point of view. As this scenario mostly results in somewhat tangential crashes, the robot could still reach its target location in some cases</li>
</ul>
<h1><a class="anchor" id="autotoc_md10"></a>
Authors</h1>
<p>All the files in this repository belong to <a href="https://github.com/andreabradpitto">Andrea Pitto</a>.<br  />
 Contact: <a href="#" onclick="location.href='mai'+'lto:'+'s39'+'42'+'710'+'@s'+'tud'+'en'+'ti.'+'un'+'ige'+'.i'+'t'; return false;">s3942710@studenti.unige.it</a>. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="http://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.0
</small></address>
</body>
</html>
