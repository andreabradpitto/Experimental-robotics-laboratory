<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Experimental robotics laboratory - Assignment 3: Experimental robotics laboratory - Assignment 3</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="dog.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Experimental robotics laboratory - Assignment 3
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Experimental robotics laboratory - Assignment 3 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_other_mainpage"></a> </p>
<h1><a class="anchor" id="autotoc_md1"></a>
Introduction</h1>
<p>This is the repository for the third and last assignment of the Robotics engineering "Experimental robotics laboratory" course, a.y. 2020/2021, held in the University of Genoa. This work is focused on the implemetation of a ROS-based finite state machine (FSM) nested within a more complex architecture. This project has been coded with <a href="https://www.python.org/download/releases/2.7/">Python 2.7</a> and it has been tested on <a href="http://wiki.ros.org/kinetic">ROS Kinetic</a>.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md3"></a>
Setting</h1>
<p>Like for the previous assignment, <a href="https://github.com/andreabradpitto/Experimental-robotics-laboratory/tree/main/assignment2">Assignment 2</a>, the main subject of the project is a robotic dog. This time, it is free to wander inside a house, comprising 6 different rooms: the entrance, the closet, the living room, the kitchen, the bathroom, and the bedroom. Each of these room locations is schematically represented by a colored ball lying on the ground, so that the dog can recognize them. In order to avoid ambiguities, each ball has a different color; following the same order mentioned above, the colors are: blue, red, green, yellow, magenta and black. Event though the house is not procedurally generated (i.e. it is always the same model), every time the simulation is started, the robot does not know where these room are, so it has to learn the house planimetry by exploration. The only thing it knows, indeed, is the association of each colored ball to its respective room, so that when the robotic dog sees e.g. the yellow ball, it knows that that room is the kitchen.<br  />
 <br  />
 Inside the house there is, other than the walls defining each room's border, also a human mannequin model, which represents the dog owner. The robot is in fact listening for human orders, and eager to play with them. The dog, in normal conditions, wanders randomly around the house, looking for undiscovered colored balls; when these are spotted, it reaches them and stores the coordinates of the corresponding room, so that this info can be used later on. When the human asks the robot to play (ideally, by actually telling the robot the word "play"), the dog reaches them, awaits for the human to tell a room name, and then it tries to reach it. Of course, diferent behaviors have been implmented whether the robot already knows the location it has been instructed to reach or not. When the robotic dog reaches the required room, it outputs a vocal acknowledgement, and finally gets back to the owner. When the robot gets tired, it gets back to its charging dock (also referred as "home") and waits for the battery to recharge. This behavior is the real life pet equivalent of sleeping; this parallelism is the one responsible for the state's actual name, as it will be shown later on in this document. The last behavior is the one devoted to the house exploration, for when the robot is looking for a precise still-to-be-discovered ball: this happens we the human wants it to reach a room that is not yet present in its database.<br  />
 <br  />
 The code implemented will show a real-time simulation of the assignment over both <a href="http://gazebosim.org/">Gazebo</a> and <a href="http://wiki.ros.org/rviz">RViz</a>. The human model and the balls will be present only on the former, while the latter is the one that will show the robotic dog's perspective, i.e. all the explored map so far, and the obstacles (the walls or the human model) that it discovered. Gazebo will always show the whole house, so that it is possbile to check if the robot knowledge matches the real planimetry of the environment. In addition, the voice messages shared between the human and the robotic dog will be displayed on terminal.</p>
<h2><a class="anchor" id="autotoc_md4"></a>
The house</h2>
<p>Here is a picture of the house taken from Gazebo:</p>
<div class="image">
<img src="house.png" alt=""/>
</div>
<p>It is comprised inside a 10x10 square, centered a the origin of the three axes. The environment can be thus effectively reduced to a bidimensional scenario, spreading over the xy plane (or, equivalently, on the z = 0 one). The robot initial position, which is also assumed to be its home location, is set in the launch file to be near the human mannequin by default: the latter is in *(-6, 8.5)*, while the former is in *(-5, 8)*. Please notice that in the above picture neither the origin axes nor the floor grid is being shown, in order to only present the house model itself; anyway, Gazebo automatically shows both of them at every new run.<br  />
 In the top left corner of the screenshot, it is possible to see the human mannequin, near another decorative object, both colored to white in order to hinder the robot's room detection feature. The robotic dog is present too, and its model is described in detail in the next subsection.</p>
<h2><a class="anchor" id="autotoc_md5"></a>
The robotic dog</h2>
<p>The robot I implemented is composed by basic geometric shapes, while trying to find a medium between looks and functionality. Indeed, it features two eyes, in order to resemble a real dog, but only one of them holds a sensor, i.e. a camera, which is used to spot the colored balls: the other one has no purpose other than giving some flavour to the model. The other sensor present on the robot is a laser scanner, which is mounted on its main chassis and is used to detect the obstacles in front of it. It is positioned much lower with respect to the camera in order to guarantee the best obstacle detection as possible: the tests I made clearly showed that having the two robot "eyes" as the two sensor spots was much harder to implement successfully.<br  />
 Here is a picture the robotic dog (again, taken from Gazibo):</p>
<div class="image">
<img src="robot.png" alt=""/>
</div>
<p>It is a two-wheeled differential drive robot. Apart from the wheels, all the joints are fixed, so the dog cannot, for intstance, rotate its head to look around. Notice the orange cube on the front, which is the shape representing the above mentioned laser scanner.</p>
<h2><a class="anchor" id="autotoc_md6"></a>
The environments</h2>
<p>Apart from the window showing what the robot sees in real time, this package launches two enviroments: Gazebo, which is used to simulate the physics of the robot, and features nice visuals. The other one is Rviz, which is simply a visualization tool, but that is able to show all the topics involved in the package. One of the most important difference between the two, atleast in this package, is that while in Gazebo one can watch the whole house and the robot with no constraints, in RViz I made so that one only sees what the robot knows and has discovered so far:</p>
<div class="image">
<img src="RViz.png" alt=""/>
</div>
<p>On the left hand side of the image above, there are many the elements useful to track and assess the robot localization, mapping and exploration capabilities. The image in the middle also shows a lot of information:</p><ul>
<li>the black elements are the obstacles detected by the robot</li>
<li>the yellow line is the global navigation plan</li>
<li>the green line is the local navigation plan</li>
<li>the red lines are the current detections of the robot's laser</li>
<li>the blue borders (and the green spheres, not shown here) are elements used by the exploration algorithm (see later on)</li>
<li>the two white squares centered over the robot are the gloabl and local costmaps <br  />
</li>
</ul>
<p>The computed world map is acquired by reading from the <code>map</code> topic, which is published by the <a href="http://wiki.ros.org/gmapping">gmapping</a> laser-based <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">SLAM</a> algorithm. It uses the laser scan data in order to chart the house walls.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md8"></a>
Architecture</h1>
<div align="center"> <img src="architecture.png" alt="" width="900" class="inline"/> </div><p>(Please notice that, in the picture, the two environments are not reported with their full input for the sake of simplicity)<br  />
 <br  />
</p>
<p>This is the list of the components that have been coded for the assignment:</p><ul>
<li><em>Human</em>: it is implemented by <a href="human_8py.html">human.py</a>, that is used to simulate the dog owner, which randomly decides to play with the robotic dog. The human checks if the robot is able to play, waits for it to come nearby, then orders it to move to a random room of the house, and finally waits for it come back. The game ends when the robotic dog is tired. All the communications sent to the robot are carried out via action a message publisher (over <code>play_topic</code>), and all the orders are handled by <a href="dog__fsm_8py.html">dog_fsm.py</a></li>
<li><em>Dog FSM</em>: it is implemented by <a href="dog__fsm_8py.html">dog_fsm.py</a>, which is used to handle the robotic dog's FSM internal architecture</li>
<li><em>Dog vision</em>: it is implemented by <a href="dog__vision_8py.html">dog_vision.py</a>, and constitutes a vision module for the robotic dog that uses OpenCV in order to constantly scan the surroundings, looking for specific colored balls. This node is able to take control, when needed, of the robot movements, allowing it to reach a room when a corresponding new ball is discovered. It also stores the positions of the balls discovered, thus learning the displacement of the rooms inside the house as time passes</li>
<li><em>Ball server</em>: it is implemented by <a href="ball__server_8py.html">ball_server.py</a>; it is a server providing the coordinates of the ball whose color matches the input room. The received room must have already been coded in a specific single digit integer format in order to be correctly parsed</li>
<li><em>A modified version of the <a href="http://wiki.ros.org/explore_lite">explore_lite</a> package</em>: it is implemented by <a href="explore_8cpp.html">explore.cpp</a>, <a href="costmap__client_8cpp.html">costmap_client.cpp</a>, and <a href="frontier__search_8cpp.html">frontier_search.cpp</a>, along with their headers: <a href="explore_8h.html">explore.h</a>, <a href="costmap__client_8h.html">costmap_client.h</a>, <a href="frontier__search_8h.html">frontier_search.h</a>, <a href="costmap__tools_8h.html">costmap_tools.h</a>. It is the algorithm allowing the robot to explore the house during the <b>Find</b> state. I made some adjustments in order to let <a href="dog__vision_8py.html">dog_vision.py</a> and <a href="dog__fsm_8py.html">dog_fsm.py</a> send service calls in order to (re-)start or stop the exploration<br  />
</li>
</ul>
<div class="image">
<img src="fsm.png" alt=""/>
</div>
<p>The dog starts in the <b>Sleep</b> state.<br  />
 In the <b>Sleep</b> state, the robot goes to sleep once he gets back home. It relies on the <a href="http://wiki.ros.org/move_base">move_base</a> algorithm in order to move in the environment, and it does so by implementing an action client and asking it to reach the coordinates corresponding to home.<br  />
 In the <b>Normal</b> state, the robot wanders randomly, by feeding the move_base algorithm with randomly generated positions. If, while moving around, the robot detects a new room/ball, it reaches it and stores the corresponding position. At any time, a play command can be received by the human: if so happens, the robotic dog transitions to the Play state. Every newly detected room, along with its subsequent data collection process, consumes robot battery. If the battery gets depleted, the robot goes to sleep, by transitioning to the <b>Sleep</b> state.<br  />
 In the <b>Play</b> state, using the move_base algorithm, the robotic dog, gets back home (i.e. close to the human), then starts listening for the room request. Once received, it checks via a service implemented in <a href="ball__server_8py.html">ball_server.py</a> which are the corresponing ball coordinates to reach. If the ball is not yet in the dog's database, it shifts to the Find state. If the ball has been seen before the dog starts moving to the chosen room, still relying on move_base. After reaching that position, it comes back to the user, again via move_base: whenever this process is completed, some battery is consumed. Furthermore, I assumed that a single cycle of this state consumes slightly more battery, on average, than one of the <b>Normal</b> state; for this reason, the threshold of remaining battery before going back to the <b>Normal</b> state is higher than in the previous case (i.e. from <b>Normal</b> to <b>Sleep</b>): this also allows me to make sure that the robot can go to sleep before completely depleting its battery.<br  />
 Lastly, in the <b>Find</b> state, the robotic dog looks for the goal ball, determined in the <b>Play</b> state. In order to do so, this state relies on the explore_lite algorithm; a service client sends a flag to the server, which corresponds to the signal the server itself is waiting it order to run the above mentioned algorithm. The code of the algorithm, which is contained inside the Explore class (see <a href="explore_8cpp.html">explore.cpp</a>), has been adjusted with the inclusion of a method able to handle requests coming from this node. The exploration algorithm keeps running until <a href="dog__vision_8py.html">dog_vision.py</a> stops its execution, i.e. a new ball has been found. If the new ball is indeed the goal one, the robot eventually gets back to the <b>Play</b> state. The human will then immediately call the dog back to its position, and another <b>Play</b> state cycle can begin.<br  />
 <br  />
</p>
<p>The usage of <a href="http://wiki.ros.org/rqt_graph">rqt_graph</a> shows all the nodes, topics and namespace involved:</p>
<div align="center"> <img src="rqt_graph_nodes+topics.png" alt="" width="900" class="inline"/> </div><p>There is only a single topic directly generated by this package:</p><ul>
<li><code>play_topic</code>: it is used by the human node in order to communicate the willingness to play, and then also the room the robot should reach. The finiste state machine node subscribes to this topic, and so it is the one devoted to handling human requests.<br  />
 <br  />
</li>
</ul>
<p>The message types used are:</p>
<ul>
<li><code>assignment3.msg/Coordinates</code>: message made of two integers <b>x</b> and <b>y</b></li>
</ul>
<p>An already-defined message could have also been used, but this new type creation had also been created as an extra exercise, in order to get more acquainted with the ROS environment.<br  />
 <br  />
</p>
<p>The services message types used are:</p>
<ul>
<li><code>assignment3.srv/Explore</code>: a simple <a href="http://docs.ros.org/en/melodic/api/std_msgs/html/msg/Int64.html">int64</a> input and output service that is used to send (re-)start and stop request to the exploration algorithm</li>
<li><code>assignment3.srv/BallService</code>: used by <a href="dog__fsm_8py.html">dog_fsm.py</a> in order to ask for the goal ball's coordinates. If the answer (of type Coordintates) is (100, 100), the requested ball is still to be found, i.e. its coordinates are not yet present in the robotic dog's database. The server is <a href="ball__server_8py.html">ball_server.py</a><br  />
 <br  />
 Finally, here are the parameters (strictly related to the robotic dog) loaded in the ROS parameter server:</li>
<li><code>state</code>: parameter specifying robot current state</li>
<li><code>map/x_max</code>: parameter specifying the maximum x-axis value for the map</li>
<li><code>map/y_max</code>: parameter specifying the maximum y-axis value for the map</li>
<li><code>map/x_min</code>: parameter specifying the minimum x-axis value for the map</li>
<li><code>map/y_min</code>: parameter specifying the minimum y-axis value for the map</li>
<li><code>home/x</code>: parameter specifying robot home position (x coordinate)</li>
<li><code>home/y</code>: parameter specifying robot home position (y coordinate)</li>
<li><code>sim_scale</code>: parameter used to scale simulation velocity</li>
<li><code>new_ ball_detected</code>: parameter used to specify whether a new ball has been detected or not</li>
<li><code>unknown_ball</code>: parameter used to identify which ball has to be looked for</li>
<li><code>room_list</code>: list of the available rooms</li>
<li><code>play_task_status</code>: parameter used to specify <b>Play</b> state progress<br  />
 A value of 0 means that the state is not active or at in initialization phase<br  />
 A 1 stands for it being in progress<br  />
 A value of 2 means that it has completed one iteration<br  />
 The last value is also used by the <b>Find</b> state, in case the location of the room asked by the human was not present in the robot's database</li>
<li><code>blue/x</code>: parameter used to specify the x coordinate of the blue ball, once discovered</li>
<li><code>blue/y</code>: parameter used to specify the y coordinate of the blue ball, once discovered</li>
<li><code>red/x</code>: parameter used to specify the x coordinate of the red ball, once discovered</li>
<li><code>red/y</code>: parameter used to specify the y coordinate of the red ball, once discovered</li>
<li><code>green/x</code>: parameter used to specify the x coordinate of the green ball, once discovered</li>
<li><code>green/y</code>: parameter used to specify the y coordinate of the green ball, once discovered</li>
<li><code>yellow/x</code>: parameter used to specify the x coordinate of the yellow ball, once discovered</li>
<li><code>yellow/y</code>: parameter used to specify the y coordinate of the yellow ball, once discovered</li>
<li><code>magenta/x</code>: parameter used to specify the x coordinate of the magenta ball, once discovered</li>
<li><code>magenta/y</code>: parameter used to specify the y coordinate of the magenta ball, once discovered</li>
<li><code>black/x</code>: parameter used to specify the x coordinate of the black ball, once discovered</li>
<li><code>black/y</code>: parameter used to specify the y coordinate of the black ball, once discovered</li>
</ul>
<p>All of these, as already pointed out, can be adjusted before runtime. Please notice that, as a general rule I applied, the value 100 for any of the above parameters always represents missing data or an undefined/default value for that specific parameter.<br  />
 Finally, here is the list of all the files and folders featured in this package (up to depth level 2, in order to skip <a href="https://www.doxygen.nl/index.html">Doxygen</a> resources):</p>
<div class="image">
<img src="tree.png" alt=""/>
</div>
<hr  />
<h1><a class="anchor" id="autotoc_md10"></a>
Requirements</h1>
<p>In order to run this piece of software, it is needed to have a Linux distribution that supports (and has them installed):</p><ul>
<li><a href="http://wiki.ros.org/kinetic">ROS Kinetic</a></li>
<li><a href="http://wiki.ros.org/smach">SMACH</a></li>
<li><a href="https://opencv.org/">OpenCV</a></li>
<li><a href="http://wiki.ros.org/gmapping">gmapping</a></li>
<li>A <a href="https://www.python.org/">Python</a> interpreter</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md12"></a>
Instructions</h1>
<p>It might be needed to make all the nodes executable, before actually being able to run them. So, after reaching this assignment's folder via a terminal, type:</p>
<div class="fragment"><div class="line">$ chmod +x scripts/*</div>
<div class="line">$ chmod +x src/* </div>
</div><!-- fragment --><p>In order to run the code, put the <em>assignment3</em> directory in your workspace, build, then open a terminal and type:</p>
<div class="fragment"><div class="line">$ roslaunch assignment.launch </div>
</div><!-- fragment --><p>This will start the simulation on both Gazebo and RViz.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md14"></a>
Assumptions, limitations and possible improvements</h1>
<ul>
<li>First of all, due to time constraints, I had no time completely test the code, i.e. I did not leave my program running for hours as suggested (also because my PC would have melted)</li>
<li>During <b>Play</b> the robotic dog ignores (and discards) any new ball discovery: I made this choice in order to grants human orders higher priority with respect to mapping or any other task. That said, I could then have made so that the robot would store its position when a new ball is found during <b>Play</b>, and then get back to it once the state ends</li>
<li>The robot's simulated battery is assumed to drain more rapidly while in <b>Play</b> than when in <b>Normal</b>, single-cycle-wise. This would be actually true in reality most often than not, but of course it would not always be the case. Furthermore, the <b>Find</b> state does not drain battery directly, which is a significant simplification. Anyway, in order to make sure that the robot is always able to get back to its charging station (i.e. home), and also to account for the major battery drain assumption, the robotic dog transitions from <b>Play</b> to <b>Normal</b> after a certain drain threshold is reached, and then always carries out a single <b>Normal</b> state cycle (no matter how far it goes, which is of course another simpification) before going to <b>Sleep</b> to recharge its batteries</li>
<li>The dog is assumed to start in its home position and, more importantly, the position the robot reaches when it gets close to the human (<b>Play</b> state) is again its home, as the mannequin (i.e. the human location) is close the charging station. This is somewhat "hardcoded" in the above mentioned state and, if the mannequin is moved to a location which is far from "home", a couple of lines in the code <a href="dog__fsm_8py.html">dog_fsm.py</a> must be changed. In that case, it is thus not sufficient to just adapt the parameters in the sim.launch launch file</li>
<li>If, for some reason, the robot loses track of a ball while it is reaching it, there is no way the robot can recover from there. However, the chances of this happening are very slim. A more realistic issue could be that, again while trying to reach a new ball, the robot could hit a wall, as in that situation it ignores incoming laser data. This happens when a wall corner covers the trajectory from the robot position to the ball, so that the latter is still partially visible from the dog's point of view. As this scenario mostly results in somewhat tangential crashes, the robot could still reach its target location in some cases</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md16"></a>
Authors</h1>
<p>All the files in this repository belong to <a href="https://github.com/andreabradpitto">Andrea Pitto</a>.<br  />
 Contact: <a href="#" onclick="location.href='mai'+'lto:'+'s39'+'42'+'710'+'@s'+'tud'+'en'+'ti.'+'un'+'ige'+'.i'+'t'; return false;">s3942710@studenti.unige.it</a>. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="http://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.0
</small></address>
</body>
</html>
